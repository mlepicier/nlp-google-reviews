{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuswLNL6FCIG"
   },
   "source": [
    "# Web-Scrapping Google Reviews\n",
    "***\n",
    "<p align='text-align: justify;'> Like in most project, data will not always be ready for use and clean. In this case, the data sources are the <b>Google reviews</b> and ratings of <i>Dumpling House</i> and <i>60 other restaurants</i>, located in <i>Chinatown, Toronto</i>. There are two ways to access the data. Firstly, <b>Application Programming Interface (API)</b>, such as Yelp and Google APIs. It can be accessed quite easily and quickly; however, those two resources are restricted to Developer and Business owners only. Another way is to use <b>web-scrapping</b> to collect raw unstructure data. Web-scrapping is leveraging text-based mark-up languages properties to extract data from websites.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM8MMbCRavGR"
   },
   "source": [
    "***\n",
    "## Project Set-Up\n",
    "> - Importing all the **required libraries** for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OP8oTYg_GHR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyyTtWcjaXb5"
   },
   "source": [
    "***\n",
    "## Web-Scrapping Excercise\n",
    "> <p align='text-align: justify;'> Performing <b>automation</b> by coding a <i>Web-Crawling Bot</i> allows to collect a very large amount of data online while saving some time, it takes only around 4 to 5 hours to run the web-scrapping model below (without any human presence needed). </p>\n",
    ">\n",
    "> ***\n",
    "> ### Scrapping URLs\n",
    "> First and foremost we need to **scrap the URLs** of the 60 restaurants we want to analyze in Chinatown, Toronto. In this case represented by being the closest in the area of *The Dumpling House*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url_set, i, url_dict):\n",
    "    for result in url_set:\n",
    "        result_name = result.find('a', class_='a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd')[\"aria-label\"]\n",
    "        result_url = result.find('a', class_='a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd')[\"href\"]\n",
    "        url_dict['Restaurant name'].append(result_name)\n",
    "        url_dict['Restaurant URL'].append( result_url)\n",
    "\n",
    "def turn_to_df(url_dict):\n",
    "    df_url=pd.DataFrame(url_dict)\n",
    "    df_url.to_csv('data/df_url.csv')\n",
    "    print(df_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7g49YT1BKeP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WEB SCRAPPING GOOGLE RESTAURANTS URLs\n",
    "'SELENIUM - BEAUTIFUL SOUP : Classic Web Scrapping'\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "url_dict = {'Restaurant name': [],'Restaurant URL': []}\n",
    "path = r\"C:\\Users\\...\\PROJECT\\CHROMEDRIVER\\chromedriver.exe\" \n",
    "driver = webdriver.Chrome(path)\n",
    "timeout = 15\n",
    "\n",
    "area = \"https://www.google.ca/maps/search/Restaurants/@43.6540536,-79.3996533,16.48z/data=!3m1!4b1!4m8!2m7!3m5!2sDumpling+House+Restaurant!3s0x882b34c396514419:0xd36007a96d7731be!4m2!1d-79.3986693!2d43.6538068!6e5\"\n",
    "driver.get(area)\n",
    "\n",
    "for i in range (1,4):\n",
    "    element_present = EC.visibility_of_all_elements_located((By.XPATH,'/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button'))\n",
    "    WebDriverWait(driver, timeout).until(element_present)\n",
    "    sleep(2) # extra-cautiousness\n",
    "    pane = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[1]/div[3]/div/a') \n",
    "    total_number_of_restaurant=4\n",
    "    \n",
    "    # Scroll as many times as necessary to load all reviews\n",
    "    for i in range(0,total_number_of_restaurant):\n",
    "            pane.send_keys(Keys.END)\n",
    "            sleep(1) \n",
    "    \n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    urls = response.find_all('div', class_='V0h1Ob-haAclf OPZbO-KE6vqe o0s21d-HiaYvf')\n",
    "    \n",
    "    get_url(urls, i, url_dict)\n",
    "    \n",
    "    button_next = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[2]/div/div[1]/div/button[2]')\n",
    "    button_next.click()\n",
    "    \n",
    "turn_to_df(url_dict)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQx2kP8UanJy"
   },
   "source": [
    ">***\n",
    "> ### Scrapping Reviews\n",
    "> Then we can proceed to the scrapping of all the **Google Reviews** of each restaurant to build our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_summary(result_set, rev_dict, name):\n",
    "    for result in result_set:\n",
    "        review_rate = result.find('span', class_='ODSEW-ShBeI-H1e3jb')[\"aria-label\"]\n",
    "        review_time = result.find('span',class_='ODSEW-ShBeI-RgZmSc-date').text\n",
    "        review_text = result.find('span',class_='ODSEW-ShBeI-text').text\n",
    "        rev_dict['Review Rate'].append(review_rate)\n",
    "        rev_dict['Review Time'].append(review_time)\n",
    "        rev_dict['Review Text'].append(review_text)\n",
    "        rev_dict['Restaurant name'].append(name)\n",
    "\n",
    "def turn_to_df(rev_dict):\n",
    "    df_reviews=pd.DataFrame(rev_dict)\n",
    "    df_reviews.to_csv('data/df_reviews.csv')\n",
    "    return(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC8xKNP0STyk"
   },
   "outputs": [],
   "source": [
    "# WEB SCRAPPING GOOGLE REVIEWS\n",
    "'SELENIUM - BEAUTIFUL SOUP : Classic Web Scrapping'  \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "\n",
    "rev_dict = {'Restaurant name' : [],'Review Rate': [],'Review Time': [],'Review Text' : []}\n",
    "df_url = pd.read_csv('data/df_url.csv')\n",
    "\n",
    "for i in range(0,1):\n",
    "    gmaps_url = df_url.loc[i,\"Restaurant URL\"] \n",
    "    driver.get(gmaps_url) # navigate to web page\n",
    "    \n",
    "    element_present = EC.visibility_of_all_elements_located((By.XPATH,'/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]/span[1]/button'))\n",
    "    WebDriverWait(driver, timeout).until(element_present)\n",
    "    button_rev = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]/span[1]/button')\n",
    "    button_rev.click()\n",
    "    sleep(2) # extra-cautiousness\n",
    "    \n",
    "    element_present = EC.visibility_of_all_elements_located((By.XPATH,'/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[7]/div[2]/button'))\n",
    "    WebDriverWait(driver, timeout).until(element_present)\n",
    "    button_sort = driver.find_element_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[7]/div[2]/button')\n",
    "    button_sort.click()\n",
    "    sleep(2)\n",
    "    \n",
    "    button_newest = driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[1]/ul/li[2]/div[3]/div[1]')\n",
    "    driver.execute_script(\"arguments[0].click();\", button_newest)\n",
    "    sleep(1)\n",
    "    \n",
    "    # Find the total number of reviews\n",
    "    total_number_of_reviews = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[2]/div/div[2]/div[2]').text.split(\" \")[0]\n",
    "    total_number_of_reviews = int(total_number_of_reviews.replace(',','')) if ',' in total_number_of_reviews else int(total_number_of_reviews)\n",
    "\n",
    "    # Find scroll layout\n",
    "    scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "    # Scroll as many times as necessary to load all reviews\n",
    "    for j in range(0,(round(total_number_of_reviews/10 - 1))):\n",
    "            driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "            sleep(3)\n",
    "\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews = response.find_all('div', class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "    \n",
    "    name = df_url.loc[i,\"Restaurant name\"]\n",
    "    get_review_summary(reviews, rev_dict, name)\n",
    "\n",
    "turn_to_df(rev_dict)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qyyTtWcjaXb5",
    "HLRepR2Bajuj",
    "CQx2kP8UanJy"
   ],
   "name": "BDA-REVIEWMINING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
